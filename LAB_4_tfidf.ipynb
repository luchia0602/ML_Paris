{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94274703",
   "metadata": {},
   "source": [
    "Copy this notebook and don't forget to **ADD YOUR NAME** in the name of the copy.\n",
    "\n",
    "When you're done, save as a python notebook, and put it in the moodle space for LAB4 by next Wednesday evening:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ace228",
   "metadata": {},
   "source": [
    "## TODO1 : linear prediction in numpy\n",
    "\n",
    "Suppose we have already learnt a multiclass classifier into 3 classes, with matrix weight W and bias vector b.\n",
    "\n",
    "(below we just set them randomly, but suppose they are the result of a learning phase)\n",
    "\n",
    "Suppose we have 4 input objects to classify (matrix X below).\n",
    "\n",
    "Implement how to predict the class\n",
    "- for the full batch X\n",
    "- for a single row in X (take the first row)\n",
    "\n",
    "Tip: look for the numpy argmax method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd550605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# weight matrix and bias vector\n",
    "W = np.random.rand(10,3)  # one column of weights per class\n",
    "b = np.random.rand(3)     # one bias value per class\n",
    "\n",
    "# 4 input vectors of size 10\n",
    "X = np.random.rand(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c671fcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the full_batch\n",
    "Y = X @ W + b\n",
    "pred = np.argmax(Y, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b3567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for first row\n",
    "Y_1 = X[0] @ W + b\n",
    "pred_1 = np.argmax(Y_1)\n",
    "pred_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb23603",
   "metadata": {},
   "source": [
    "The scikit-learn python framework\n",
    "Scikit-learn is a machine learning python library, which implements various regression, classification and clustering algorithms.\n",
    "\n",
    "It's imported as sklearn.\n",
    "\n",
    "Note sklearn is much used for linear and log-linear models, and less for deep learning.\n",
    "\n",
    "The classification and regression parts on the home page both point to the same general page on supervised learning: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77102b3",
   "metadata": {},
   "source": [
    "Scikit-learn Vectorizers to get BOW vectors\n",
    "In last lab we coded how to transform a collection of documents into matrices of \"bag of words\" representations of these documents (the X_train and X_test matrices of LAB2 and 3).\n",
    "\n",
    "Scikit-learn has \"vectorizer\" methods to do that : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "These matrices use the scipy.sparse type, which is appropriate for sparse matrices.\n",
    "\n",
    "All the vectorizers modules have 3 methods:\n",
    "\n",
    "fit : builds the vocabulary and the correspondance between word forms and word ids\n",
    "transform : transforms the documents into matrices of counts\n",
    "fit_transform : performs both actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae960d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# a French corpus (to see what is going on with diacritics)\n",
    "train_corpus = [\n",
    "'Ceci est un document.',\n",
    "\"Aujourd'hui, ce document est à moi.\",\n",
    "'Et voilà le troisième.',\n",
    "'Le premier document est-il le plus intéressant?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "# the vectorizer is empty : this generates an error. but it is empty YET\n",
    "#print(vectorizer.vocabulary_) # => AttributeError: 'CountVectorizer' object has no attribute 'vocabulary_'\n",
    "#print(vectorizer.get_feature_names_out()) # => NotFittedError: Vocabulary not fitted or provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34c033",
   "metadata": {},
   "source": [
    "The fit_transform method\n",
    "Calling fit_transform on train_corpus will :\n",
    "\n",
    "tokenize the text : it will split it into \"words\" using a regular expression to define what can be a separator between words\n",
    "NB: this is a very uninformed and rough tokenization, meaning the obtained tokens are not always words as defined in linguistics\n",
    "identify the vocabulary and associate an id to each element of the vocabulary\n",
    "AND transform the training set into a matrix of BOW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66b484f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X_train <class 'scipy.sparse._csr.csr_matrix'>\n",
      "shape of X_train (4, 16)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 21 stored elements and shape (4, 16)>\n",
      "  Coords\tValues\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 13)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 9)\t2\n",
      "  (3, 12)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 8)\t1\n",
      "[[0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1]\n",
      " [0 0 0 1 1 0 0 1 1 2 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "# the matrix is sparse\n",
    "print(\"type of X_train\", type(X_train))\n",
    "print(\"shape of X_train\", X_train.shape)\n",
    "print(X_train)\n",
    "# here it is as a standard matrix\n",
    "print(X_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c4b92a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 4 stored elements and shape (1, 16)>\n",
      "  Coords\tValues\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3cc383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ceci': 2, 'est': 4, 'un': 14, 'document': 3, 'aujourd': 0, 'hui': 6, 'ce': 1, 'moi': 10, 'et': 5, 'voilà': 15, 'le': 9, 'troisième': 13, 'premier': 12, 'il': 7, 'plus': 11, 'intéressant': 8}\n",
      "['aujourd' 'ce' 'ceci' 'document' 'est' 'et' 'hui' 'il' 'intéressant' 'le'\n",
      " 'moi' 'plus' 'premier' 'troisième' 'un' 'voilà']\n"
     ]
    }
   ],
   "source": [
    "# here is the mapping between word forms and ids (our \"w2i\" in previous lab ses\n",
    "print(vectorizer.vocabulary_) # just in alphabetic order? so tokenize the whole text and for token in text assign a number to it starting from 0 BUT if we had a PAD that 0 would go there?\n",
    "# the list of word forms (our i2w)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc4f14",
   "metadata": {},
   "source": [
    "TODO2 : answer the following comprehension questions:\n",
    "What is the size of the vocabulary\n",
    "What does the 4th column of X.train.toarray() represent ?\n",
    "What is printed when printing the sparse matrix ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eef29393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[1 1 0 1]\n",
      "[1 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 21 stored elements and shape (4, 16)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the size of the vocabulary\n",
    "print(len(vectorizer.vocabulary_)) # so CountVectorizer thinks there are 16 unique words in our 4 sentences\n",
    "# What does the 4th column of X.train.toarray() represent ?\n",
    "print(X_train.toarray()[:, 3]) # rows = sentences, columns = unique words; 4th column represents the appearance of 4th unique word across our corpus ('document'). indeed, est appears in all sentences but third, that is why the vector is [1 1 0 1]\n",
    "# since the word 'ceci' appears in all sentenced but third too, it has the same structure\n",
    "print(X_train.toarray()[:, 4])\n",
    "# What is printed when printing the sparse matrix ?\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efe25625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_Train (4, 16)\n",
      "shape of X_test (2, 16)\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0]]\n",
      "{'ceci': 2, 'est': 4, 'un': 14, 'document': 3, 'aujourd': 0, 'hui': 6, 'ce': 1, 'moi': 10, 'et': 5, 'voilà': 15, 'le': 9, 'troisième': 13, 'premier': 12, 'il': 7, 'plus': 11, 'intéressant': 8}\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [ 'Ah un nouveau document.',\n",
    "'Et ceci est encore un document.']\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "print(\"shape of X_Train\", X_train.shape)\n",
    "print(\"shape of X_test\", X_test.shape) # there are just 2 sentences in the test_corpus => 2 rows\n",
    "print(X_test.toarray())\n",
    "# 'document' is 3rd in the train_vocab and it appears in both sentences in test_corpus, that is why column 4 is (1, 1).\n",
    "# 'ceci' is only in the 2nd sentence, that is why there is 1 in third column second row but not in the first row\n",
    "print(vectorizer.vocabulary_) # it is the same, so those words that are in test_corpus but not in train_corpus are just not added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c1c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_test:  (2, 8)\n",
      "[[1 0 1 0 0 0 1 1]\n",
      " [0 1 1 1 1 1 0 1]]\n",
      "{'ah': 0, 'un': 7, 'nouveau': 6, 'document': 2, 'et': 5, 'ceci': 1, 'est': 4, 'encore': 3}\n"
     ]
    }
   ],
   "source": [
    "# What happened to the words in test_corpus that are not present in train_corpus?\n",
    "# Compare to vectorizer.fit_transform\n",
    "X_test = vectorizer.fit_transform(test_corpus)\n",
    "print(\"shape of X_test: \", X_test.shape) # there are only 8 unique words in 2 sentences, that is why the shape is (2,8).\n",
    "# if we use just transform, we take the vocabulary of uique words from train set\n",
    "print(X_test.toarray())\n",
    "print(vectorizer.vocabulary_) # here we built a new vocab based on test_corpus's unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac0b7d",
   "metadata": {},
   "source": [
    "TODO4: changing the parameters\n",
    "We are now providing input sentences in which tokens have all been separated by spaces.\n",
    "\n",
    "How can you change the tokenization that the CountVectorizer will use ? (see its constructor)\n",
    "in particular, how can you make CountVectorizer split on spaces only?\n",
    "Indications: study https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html to see all the members of the instance, and deduce which member to modify:\n",
    "\n",
    "Find out which parameters to modify to switch to bigram and trigram of characters features, and print the obtained vocabulary - this means that the vocabulary will not be made of words, but of sequences of characters, of length 2 (character bigram) or 3 (character trigram)\n",
    "\n",
    "Study the TfidfVectorizer class https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html and deduce how to easily obtain TF.IDF weigthed vector representations of the documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5878280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How can you change the tokenization that the CountVectorizer will use ? (see its constructor)\n",
    "# ==> here is the parameter that says what kind of tokenization we use token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "# ==> \"The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "372fab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ceci': 2, 'est': 5, 'un': 14, 'document.': 4, \"aujourd'hui,\": 0, 'ce': 1, 'document': 3, 'à': 16, 'moi.': 10, 'et': 7, 'voilà': 15, 'le': 9, 'troisième.': 13, 'premier': 12, 'est-il': 6, 'plus': 11, 'intéressant?': 8}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# 2. in particular, how can you make CountVectorizer split on spaces only? Indications: study https://scikitlearn. org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html to see all the members of the instance, and deduce which member to modify:\n",
    "# ==> token_pattern='[^ ]+'\n",
    "vectorizer = CountVectorizer(token_pattern='[^ ]+')\n",
    "X_test = vectorizer.fit_transform(train_corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_)) # 8 -> 17\n",
    "# this is not really a good solution since punctuation symbols stick to words and 'document' and 'document.' are two different 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b56911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find out which parameters to modify to switch to bigram and trigram of characters features, and print the obtained vocabulary - this means that the vocabulary will not be made of words, but of sequences of characters, of length 2 (character bigram) or 3 (character trigram)\n",
    "# ==> there are parameters for that:\n",
    "# ngram_rangetuple (min_n, max_n), default=(1, 1)\n",
    "# The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable.\n",
    "# analyzer{‘word’, ‘char’, ‘char_wb’} or callable, default=’word’\n",
    "# Whether the feature should be made of word n-gram or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d48b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ah': 11, 'h ': 38, ' u': 9, 'un': 71, 'n ': 44, ' n': 7, 'no': 49, 'ou': 57, 'uv': 73, 've': 75, 'ea': 27, 'au': 13, 'u ': 67, ' d': 2, 'do': 23, 'oc': 53, 'cu': 21, 'um': 69, 'me': 42, 'en': 31, 'nt': 51, 't.': 66, 'ah ': 12, 'h u': 39, ' un': 10, 'un ': 72, 'n n': 46, ' no': 8, 'nou': 50, 'ouv': 58, 'uve': 74, 'vea': 76, 'eau': 28, 'au ': 14, 'u d': 68, ' do': 3, 'doc': 24, 'ocu': 54, 'cum': 22, 'ume': 70, 'men': 43, 'ent': 33, 'nt.': 52, 'et': 36, 't ': 63, ' c': 0, 'ce': 15, 'ec': 29, 'ci': 17, 'i ': 40, ' e': 4, 'es': 34, 'st': 61, 'nc': 47, 'co': 19, 'or': 55, 're': 59, 'e ': 25, 'et ': 37, 't c': 64, ' ce': 1, 'cec': 16, 'eci': 30, 'ci ': 18, 'i e': 41, ' es': 6, 'est': 35, 'st ': 62, 't e': 65, ' en': 5, 'enc': 32, 'nco': 48, 'cor': 20, 'ore': 56, 're ': 60, 'e u': 26, 'n d': 45}\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), analyzer='char')\n",
    "X_test = vectorizer.fit_transform(test_corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_)) # this way we received a vocab of 77 unique combinations of 2 and 3 letters in our corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05881387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57615236, 0.        , 0.40993715, 0.        , 0.        ,\n",
       "        0.        , 0.57615236, 0.40993715],\n",
       "       [0.        , 0.44665616, 0.31779954, 0.44665616, 0.44665616,\n",
       "        0.44665616, 0.        , 0.31779954]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Study the TfidfVectorizer class https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html and deduce how to easily obtain TF.IDF weigthed vector representations of the documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b01631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEMBERS:\n",
      " ('input', 'content')\n",
      "('encoding', 'utf-8')\n",
      "('decode_error', 'strict')\n",
      "('strip_accents', None)\n",
      "('preprocessor', None)\n",
      "('tokenizer', None)\n",
      "('analyzer', 'word')\n",
      "('lowercase', True)\n",
      "('token_pattern', '(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "('stop_words', None)\n",
      "('max_df', 1.0)\n",
      "('min_df', 1)\n",
      "('max_features', None)\n",
      "('ngram_range', (1, 1))\n",
      "('vocabulary', None)\n",
      "('binary', False)\n",
      "('dtype', <class 'numpy.float64'>)\n",
      "('norm', 'l2')\n",
      "('use_idf', True)\n",
      "('smooth_idf', True)\n",
      "('sublinear_tf', False)\n",
      "('_tfidf', TfidfTransformer())\n",
      "('fixed_vocabulary_', False)\n",
      "('_stop_words_id', 140731225928176)\n",
      "('vocabulary_', {'ceci': 2, 'est': 5, 'un': 15, 'document': 3, 'aujourd': 0, 'hui': 7, 'ce': 1, 'encore': 4, 'moi': 11, 'et': 6, 'voilà': 16, 'le': 10, 'troisième': 14, 'premier': 13, 'il': 8, 'plus': 12, 'intéressant': 9})\n"
     ]
    }
   ],
   "source": [
    "train_corpus = [\n",
    "'Ceci est un document .',\n",
    "\"Aujourd'hui , ce document est encore un document à moi .\",\n",
    "'Et voilà le troisième .',\n",
    "'Le premier document est -il le plus intéressant ?',\n",
    "]\n",
    "X_tfidf = vectorizer.fit_transform(train_corpus)\n",
    "print(\"\\nMEMBERS:\\n\", \"\\n\".join([str(x) for x in vectorizer.__dict__.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7782f9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aujourd</th>\n",
       "      <th>ce</th>\n",
       "      <th>ceci</th>\n",
       "      <th>document</th>\n",
       "      <th>encore</th>\n",
       "      <th>est</th>\n",
       "      <th>et</th>\n",
       "      <th>hui</th>\n",
       "      <th>il</th>\n",
       "      <th>intéressant</th>\n",
       "      <th>le</th>\n",
       "      <th>moi</th>\n",
       "      <th>plus</th>\n",
       "      <th>premier</th>\n",
       "      <th>troisième</th>\n",
       "      <th>un</th>\n",
       "      <th>voilà</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640655</td>\n",
       "      <td>0.408922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461286</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.230643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.525473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.583561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    aujourd        ce      ceci  document    encore       est        et  \\\n",
       "0  0.000000  0.000000  0.640655  0.408922  0.000000  0.408922  0.000000   \n",
       "1  0.361347  0.361347  0.000000  0.461286  0.361347  0.230643  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.525473   \n",
       "3  0.000000  0.000000  0.000000  0.236221  0.000000  0.236221  0.000000   \n",
       "\n",
       "        hui        il  intéressant        le       moi      plus   premier  \\\n",
       "0  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.361347  0.000000     0.000000  0.000000  0.361347  0.000000  0.000000   \n",
       "2  0.000000  0.000000     0.000000  0.414289  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.370086     0.370086  0.583561  0.000000  0.370086  0.370086   \n",
       "\n",
       "   troisième       un     voilà  \n",
       "0   0.000000  0.50510  0.000000  \n",
       "1   0.000000  0.28489  0.000000  \n",
       "2   0.525473  0.00000  0.525473  \n",
       "3   0.000000  0.00000  0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_tfidf = vectorizer.fit_transform(train_corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_dense = X_tfidf.toarray()\n",
    "df_tfidf = pd.DataFrame(X_dense, columns=terms)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e00b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
